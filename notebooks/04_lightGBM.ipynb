{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "header",
      "metadata": {},
      "source": [
        "# LightGBM Model for Credit Risk Prediction\n",
        "\n",
        "This notebook implements LightGBM to compare with the Decision Tree model from the paper.\n",
        "\n",
        "**Key Results:**\n",
        "- Best F1-Score: **0.550** (4.5% improvement over Decision Tree)\n",
        "- Best ROC-AUC: **0.779** (4.1% improvement over Decision Tree)\n",
        "- Most important feature: **PAY_0** (repayment status in September)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "import-header",
      "metadata": {},
      "source": [
        "### Import libraries and dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "imports",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report, confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "load-data",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_original = pd.read_csv('../data/UCI_Credit_Card.csv')\n",
        "df = df_original.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "data-prep-header",
      "metadata": {},
      "source": [
        "### Data Preparation\n",
        "\n",
        "Using the same preprocessing steps as the Decision Tree model for consistency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "handle-outliers",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Records after outlier handling: 30000\n"
          ]
        }
      ],
      "source": [
        "# Cap outliers at 1st and 99th percentile for bill and payment amounts\n",
        "bill_payment_cols = ['BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', \n",
        "                     'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1', 'PAY_AMT2', \n",
        "                     'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']\n",
        "\n",
        "for col in bill_payment_cols:\n",
        "    lower_bound = df[col].quantile(0.01)\n",
        "    upper_bound = df[col].quantile(0.99)\n",
        "    df[col] = df[col].clip(lower=lower_bound, upper=upper_bound)\n",
        "\n",
        "# Filter valid ages\n",
        "df = df[(df['AGE'] >= 18) & (df['AGE'] <= 100)]\n",
        "print(f\"Records after outlier handling: {len(df)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "clean-categorical",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clean EDUCATION: recode 0, 5, 6 as 4 (others)\n",
        "df['EDUCATION'] = df['EDUCATION'].replace({0: 4, 5: 4, 6: 4})\n",
        "\n",
        "# Clean MARRIAGE: recode 0 as 3 (others)\n",
        "df['MARRIAGE'] = df['MARRIAGE'].replace({0: 3})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "feature-engineering",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Records after excluding divorced women: 29768\n"
          ]
        }
      ],
      "source": [
        "# Create Gender-Marriage combined feature (as described in the paper)\n",
        "def create_gender_marriage_category(row):\n",
        "    sex = row['SEX']\n",
        "    marriage = row['MARRIAGE']\n",
        "    \n",
        "    if sex == 1:  # Male\n",
        "        if marriage == 1:\n",
        "            return 1  # Married man\n",
        "        elif marriage == 2:\n",
        "            return 2  # Single man\n",
        "        else:\n",
        "            return 3  # Divorced man\n",
        "    else:  # Female\n",
        "        if marriage == 1:\n",
        "            return 4  # Married woman\n",
        "        elif marriage == 2:\n",
        "            return 5  # Single woman\n",
        "        else:\n",
        "            return 6  # Divorced woman\n",
        "\n",
        "df['GENDER_MARRIAGE'] = df.apply(create_gender_marriage_category, axis=1)\n",
        "\n",
        "# Exclude divorced women (category 6) as per the paper\n",
        "df = df[df['GENDER_MARRIAGE'] != 6].copy()\n",
        "print(f\"Records after excluding divorced women: {len(df)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "prepare-features",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature matrix shape: (29768, 24)\n",
            "Target vector shape: (29768,)\n",
            "Default rate: 22.1%\n"
          ]
        }
      ],
      "source": [
        "# Remove ID column\n",
        "if 'ID' in df.columns:\n",
        "    df = df.drop('ID', axis=1)\n",
        "\n",
        "# Define target and features\n",
        "target_col = 'default.payment.next.month'\n",
        "y = df[target_col].copy()\n",
        "\n",
        "feature_cols = ['LIMIT_BAL', 'SEX', 'EDUCATION', 'MARRIAGE', 'AGE',\n",
        "                'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6',\n",
        "                'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', \n",
        "                'BILL_AMT5', 'BILL_AMT6',\n",
        "                'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', \n",
        "                'PAY_AMT5', 'PAY_AMT6',\n",
        "                'GENDER_MARRIAGE']\n",
        "\n",
        "X = df[feature_cols].copy()\n",
        "\n",
        "print(f\"Feature matrix shape: {X.shape}\")\n",
        "print(f\"Target vector shape: {y.shape}\")\n",
        "print(f\"Default rate: {y.mean():.1%}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "train-test-split",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set size: 20837 (70.0%)\n",
            "Test set size: 8931 (30.0%)\n",
            "Training default rate: 22.134%\n",
            "Test default rate: 22.136%\n"
          ]
        }
      ],
      "source": [
        "# Train-test split (same as Decision Tree: 70-30 with random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.3,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Training set size: {X_train.shape[0]} ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
        "print(f\"Test set size: {X_test.shape[0]} ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
        "print(f\"Training default rate: {y_train.mean():.3%}\")\n",
        "print(f\"Test default rate: {y_test.mean():.3%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "class-imbalance-header",
      "metadata": {},
      "source": [
        "### Handle Class Imbalance\n",
        "\n",
        "Calculate scale_pos_weight for LightGBM (ratio of negative to positive samples)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "class-imbalance",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "CLASS IMBALANCE HANDLING\n",
            "==================================================\n",
            "Negative samples (no default): 16225\n",
            "Positive samples (default): 4612\n",
            "Scale pos weight: 3.52\n"
          ]
        }
      ],
      "source": [
        "# Calculate scale_pos_weight for class imbalance handling\n",
        "negative_samples = (y_train == 0).sum()\n",
        "positive_samples = (y_train == 1).sum()\n",
        "scale_pos_weight_value = negative_samples / positive_samples\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"CLASS IMBALANCE HANDLING\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Negative samples (no default): {negative_samples}\")\n",
        "print(f\"Positive samples (default): {positive_samples}\")\n",
        "print(f\"Scale pos weight: {scale_pos_weight_value:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tuning-header",
      "metadata": {},
      "source": [
        "### Hyperparameter Tuning\n",
        "\n",
        "Grid search with 10-fold cross-validation over 324 parameter combinations.\n",
        "\n",
        "**Best Parameters Found:**\n",
        "- `learning_rate`: 0.01\n",
        "- `max_depth`: 7\n",
        "- `n_estimators`: 200\n",
        "- `num_leaves`: 31\n",
        "- `min_child_samples`: 20\n",
        "\n",
        "Best cross-validation F1 score: **0.5427**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "grid-search",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "HYPERPARAMETER TUNING WITH GRID SEARCH\n",
            "==================================================\n",
            "Total combinations: 324 = 324\n",
            "Starting grid search (this may take a few minutes)...\n",
            "\n",
            "Fitting 10 folds for each of 324 candidates, totalling 3240 fits\n",
            "\n",
            "Best parameters found:\n",
            "  learning_rate: 0.01\n",
            "  max_depth: 7\n",
            "  min_child_samples: 20\n",
            "  n_estimators: 200\n",
            "  num_leaves: 31\n",
            "\n",
            "Best cross-validation F1 score: 0.5427\n"
          ]
        }
      ],
      "source": [
        "# Define the parameter grid for hyperparameter tuning\n",
        "lgb_param_grid = {\n",
        "    'num_leaves': [20, 31, 50],\n",
        "    'max_depth': [3, 5, 7, -1],\n",
        "    'learning_rate': [0.01, 0.05, 0.1],\n",
        "    'n_estimators': [100, 200, 500],\n",
        "    'min_child_samples': [20, 50, 100]\n",
        "}\n",
        "\n",
        "# Initialize LightGBM classifier with class imbalance handling\n",
        "lgb_model = lgb.LGBMClassifier(\n",
        "    random_state=42,\n",
        "    scale_pos_weight=scale_pos_weight_value,\n",
        "    verbose=-1\n",
        ")\n",
        "\n",
        "# Perform grid search with cross-validation\n",
        "print(\"=\" * 50)\n",
        "print(\"HYPERPARAMETER TUNING WITH GRID SEARCH\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Total combinations: {3 * 4 * 3 * 3 * 3} = 324\")\n",
        "print(\"Starting grid search (this may take a few minutes)...\\n\")\n",
        "\n",
        "lgb_grid_search = GridSearchCV(\n",
        "    lgb_model,\n",
        "    lgb_param_grid,\n",
        "    cv=10,\n",
        "    scoring='f1',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "lgb_grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(f\"\\nBest parameters found:\")\n",
        "for param, value in lgb_grid_search.best_params_.items():\n",
        "    print(f\"  {param}: {value}\")\n",
        "print(f\"\\nBest cross-validation F1 score: {lgb_grid_search.best_score_:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eval-header",
      "metadata": {},
      "source": [
        "### Performance Evaluation\n",
        "\n",
        "**Test Set Results:**\n",
        "| Metric | Value |\n",
        "|--------|-------|\n",
        "| Accuracy | 0.789 |\n",
        "| Precision | 0.521 |\n",
        "| Recall | 0.582 |\n",
        "| F1-Score | 0.550 |\n",
        "| ROC-AUC | 0.779 |\n",
        "\n",
        "The model correctly identifies 58.2% of actual defaulters (recall) while maintaining 52.1% precision."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "performance-eval",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "LIGHTGBM PERFORMANCE EVALUATION\n",
            "==================================================\n",
            "\n",
            "Performance Metrics:\n",
            "Accuracy:  0.789\n",
            "Precision: 0.521\n",
            "Recall:    0.582\n",
            "F1-score:  0.550\n",
            "ROC-AUC:   0.779\n",
            "\n",
            "--------------------------------------------------\n",
            "Classification Report:\n",
            "--------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  No Default       0.88      0.85      0.86      6954\n",
            "     Default       0.52      0.58      0.55      1977\n",
            "\n",
            "    accuracy                           0.79      8931\n",
            "   macro avg       0.70      0.71      0.71      8931\n",
            "weighted avg       0.80      0.79      0.79      8931\n",
            "\n",
            "--------------------------------------------------\n",
            "Confusion Matrix:\n",
            "--------------------------------------------------\n",
            "[[5896 1058]\n",
            " [ 827 1150]]\n",
            "\n",
            "True Negatives:  5896\n",
            "False Positives: 1058\n",
            "False Negatives: 827\n",
            "True Positives:  1150\n"
          ]
        }
      ],
      "source": [
        "# Get the best model from grid search\n",
        "best_lgb_model = lgb_grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on test set\n",
        "y_pred_lgb = best_lgb_model.predict(X_test)\n",
        "y_pred_proba_lgb = best_lgb_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate performance metrics\n",
        "accuracy_lgb = accuracy_score(y_test, y_pred_lgb)\n",
        "precision_lgb = precision_score(y_test, y_pred_lgb)\n",
        "recall_lgb = recall_score(y_test, y_pred_lgb)\n",
        "f1_lgb = f1_score(y_test, y_pred_lgb)\n",
        "roc_auc_lgb = roc_auc_score(y_test, y_pred_proba_lgb)\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"LIGHTGBM PERFORMANCE EVALUATION\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"\\nPerformance Metrics:\")\n",
        "print(f\"Accuracy:  {accuracy_lgb:.3f}\")\n",
        "print(f\"Precision: {precision_lgb:.3f}\")\n",
        "print(f\"Recall:    {recall_lgb:.3f}\")\n",
        "print(f\"F1-score:  {f1_lgb:.3f}\")\n",
        "print(f\"ROC-AUC:   {roc_auc_lgb:.3f}\")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 50)\n",
        "print(\"Classification Report:\")\n",
        "print(\"-\" * 50)\n",
        "print(classification_report(y_test, y_pred_lgb, target_names=['No Default', 'Default']))\n",
        "\n",
        "print(\"-\" * 50)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(\"-\" * 50)\n",
        "cm_lgb = confusion_matrix(y_test, y_pred_lgb)\n",
        "print(cm_lgb)\n",
        "print(f\"\\nTrue Negatives:  {cm_lgb[0, 0]}\")\n",
        "print(f\"False Positives: {cm_lgb[0, 1]}\")\n",
        "print(f\"False Negatives: {cm_lgb[1, 0]}\")\n",
        "print(f\"True Positives:  {cm_lgb[1, 1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "feature-imp-header",
      "metadata": {},
      "source": [
        "### Feature Importance Analysis\n",
        "\n",
        "**Top 5 Features by Gain:**\n",
        "1. **PAY_0** - Repayment status in September (dominant feature with 274,048 gain)\n",
        "2. **PAY_AMT2** - Payment amount in August (27,434 gain)\n",
        "3. **LIMIT_BAL** - Credit limit (25,512 gain)\n",
        "4. **BILL_AMT1** - Bill statement in September (24,630 gain)\n",
        "5. **PAY_4** - Repayment status in June (14,138 gain)\n",
        "\n",
        "PAY_0 is by far the most predictive feature, consistent with the paper's findings that repayment history is the strongest indicator of default risk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "feature-importance",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "FEATURE IMPORTANCE ANALYSIS\n",
            "==================================================\n",
            "\n",
            "Top 10 Features by Gain-based Importance:\n",
            "--------------------------------------------------\n",
            "  Feature  Importance (Split)  Importance (Gain)\n",
            "    PAY_0                 332      274047.755493\n",
            " PAY_AMT2                 393       27434.305377\n",
            "LIMIT_BAL                 616       25512.307361\n",
            "BILL_AMT1                 649       24629.907673\n",
            "    PAY_4                 135       14137.522610\n",
            "    PAY_2                 237       12836.259132\n",
            " PAY_AMT1                 360       12560.479429\n",
            " PAY_AMT3                 309       11943.671025\n",
            "    PAY_3                 222       11438.383887\n",
            " PAY_AMT4                 258       10268.698339\n",
            "\n",
            "\n",
            "Top 10 Features by Split-based Importance:\n",
            "--------------------------------------------------\n",
            "  Feature  Importance (Split)  Importance (Gain)\n",
            "BILL_AMT1                 649       24629.907673\n",
            "LIMIT_BAL                 616       25512.307361\n",
            " PAY_AMT2                 393       27434.305377\n",
            " PAY_AMT1                 360       12560.479429\n",
            "      AGE                 347        7192.780829\n",
            "    PAY_0                 332      274047.755493\n",
            " PAY_AMT3                 309       11943.671025\n",
            " PAY_AMT5                 295        7100.816555\n",
            "BILL_AMT3                 291        7159.977135\n",
            " PAY_AMT6                 274        5687.221326\n",
            "\n",
            "==================================================\n",
            "KEY FEATURES FROM PAPER COMPARISON\n",
            "==================================================\n",
            "\n",
            "Importance of PAY variables and AGE:\n",
            "Feature  Importance (Split)  Importance (Gain)\n",
            "  PAY_0                 332      274047.755493\n",
            "  PAY_4                 135       14137.522610\n",
            "  PAY_2                 237       12836.259132\n",
            "  PAY_3                 222       11438.383887\n",
            "  PAY_6                 236        8547.498108\n",
            "    AGE                 347        7192.780829\n",
            "  PAY_5                 153        6285.808367\n"
          ]
        }
      ],
      "source": [
        "# Feature Importance Analysis\n",
        "print(\"=\" * 50)\n",
        "print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Get feature importances (split-based by default in sklearn API)\n",
        "feature_importance_split = pd.DataFrame({\n",
        "    'Feature': X_train.columns,\n",
        "    'Importance (Split)': best_lgb_model.feature_importances_\n",
        "}).sort_values('Importance (Split)', ascending=False)\n",
        "\n",
        "# Get gain-based importance using booster\n",
        "booster = best_lgb_model.booster_\n",
        "gain_importance = booster.feature_importance(importance_type='gain')\n",
        "feature_importance_gain = pd.DataFrame({\n",
        "    'Feature': X_train.columns,\n",
        "    'Importance (Gain)': gain_importance\n",
        "}).sort_values('Importance (Gain)', ascending=False)\n",
        "\n",
        "# Combine both importance measures\n",
        "feature_importance = feature_importance_split.merge(\n",
        "    feature_importance_gain, on='Feature'\n",
        ")\n",
        "feature_importance = feature_importance.sort_values('Importance (Gain)', ascending=False)\n",
        "\n",
        "print(\"\\nTop 10 Features by Gain-based Importance:\")\n",
        "print(\"-\" * 50)\n",
        "print(feature_importance.head(10).to_string(index=False))\n",
        "\n",
        "print(\"\\n\\nTop 10 Features by Split-based Importance:\")\n",
        "print(\"-\" * 50)\n",
        "print(feature_importance.sort_values('Importance (Split)', ascending=False).head(10).to_string(index=False))\n",
        "\n",
        "# Highlight PAY variables and AGE (as mentioned in the paper)\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"KEY FEATURES FROM PAPER COMPARISON\")\n",
        "print(\"=\" * 50)\n",
        "pay_features = ['PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6', 'AGE']\n",
        "print(\"\\nImportance of PAY variables and AGE:\")\n",
        "key_features = feature_importance[feature_importance['Feature'].isin(pay_features)]\n",
        "key_features = key_features.sort_values('Importance (Gain)', ascending=False)\n",
        "print(key_features.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "comparison-header",
      "metadata": {},
      "source": [
        "### Model Comparison with Decision Tree\n",
        "\n",
        "**LightGBM vs Decision Tree Performance:**\n",
        "\n",
        "| Metric | Decision Tree | LightGBM | Improvement |\n",
        "|--------|---------------|----------|-------------|\n",
        "| Accuracy | 0.781 | 0.789 | +1.1% |\n",
        "| Precision | 0.504 | 0.521 | +3.3% |\n",
        "| Recall | 0.550 | 0.582 | +5.8% |\n",
        "| F1-Score | 0.526 | 0.550 | +4.5% |\n",
        "| ROC-AUC | 0.748 | 0.779 | +4.1% |\n",
        "\n",
        "LightGBM outperforms the Decision Tree model across all metrics, with the most significant improvement in recall (+5.8%), indicating better detection of actual defaulters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "model-comparison",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "MODEL COMPARISON: DECISION TREE VS LIGHTGBM\n",
            "============================================================\n",
            "\n",
            "Performance Comparison:\n",
            "------------------------------------------------------------\n",
            "   Metric  Decision Tree  LightGBM  Difference  % Change\n",
            " Accuracy       0.780652  0.788937    0.008286      1.06\n",
            "Precision       0.504174  0.520833    0.016659      3.30\n",
            "   Recall       0.549823  0.581689    0.031866      5.80\n",
            " F1-Score       0.526010  0.549582    0.023572      4.48\n",
            "  ROC-AUC       0.748421  0.779349    0.030927      4.13\n",
            "\n",
            "------------------------------------------------------------\n",
            "Summary:\n",
            "------------------------------------------------------------\n",
            "✓ LightGBM outperforms Decision Tree on F1-Score by 0.024 (4.5% improvement)\n",
            "✓ LightGBM outperforms Decision Tree on ROC-AUC by 0.031 (4.1% improvement)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/mizukinakano/code/Mizuki8783/ciccc-group-work/ds301-midterm/.venv/lib/python3.12/site-packages/sklearn/base.py:463: InconsistentVersionWarning: Trying to unpickle estimator DecisionTreeClassifier from version 1.7.2 when using version 1.8.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Load the Decision Tree model for comparison\n",
        "dt_model = joblib.load('./models/best_decision_tree_model.pkl')\n",
        "\n",
        "# Get Decision Tree predictions\n",
        "y_pred_dt = dt_model.predict(X_test)\n",
        "y_pred_proba_dt = dt_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Decision Tree metrics\n",
        "accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
        "precision_dt = precision_score(y_test, y_pred_dt)\n",
        "recall_dt = recall_score(y_test, y_pred_dt)\n",
        "f1_dt = f1_score(y_test, y_pred_dt)\n",
        "roc_auc_dt = roc_auc_score(y_test, y_pred_proba_dt)\n",
        "\n",
        "# Model Comparison\n",
        "print(\"=\" * 60)\n",
        "print(\"MODEL COMPARISON: DECISION TREE VS LIGHTGBM\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC'],\n",
        "    'Decision Tree': [accuracy_dt, precision_dt, recall_dt, f1_dt, roc_auc_dt],\n",
        "    'LightGBM': [accuracy_lgb, precision_lgb, recall_lgb, f1_lgb, roc_auc_lgb]\n",
        "})\n",
        "\n",
        "comparison_df['Difference'] = comparison_df['LightGBM'] - comparison_df['Decision Tree']\n",
        "comparison_df['% Change'] = (comparison_df['Difference'] / comparison_df['Decision Tree'] * 100).round(2)\n",
        "\n",
        "print(\"\\nPerformance Comparison:\")\n",
        "print(\"-\" * 60)\n",
        "print(comparison_df.to_string(index=False))\n",
        "\n",
        "print(\"\\n\" + \"-\" * 60)\n",
        "print(\"Summary:\")\n",
        "print(\"-\" * 60)\n",
        "if f1_lgb > f1_dt:\n",
        "    print(f\"✓ LightGBM outperforms Decision Tree on F1-Score by {(f1_lgb - f1_dt):.3f} ({((f1_lgb - f1_dt) / f1_dt * 100):.1f}% improvement)\")\n",
        "else:\n",
        "    print(f\"✗ Decision Tree outperforms LightGBM on F1-Score by {(f1_dt - f1_lgb):.3f}\")\n",
        "\n",
        "if roc_auc_lgb > roc_auc_dt:\n",
        "    print(f\"✓ LightGBM outperforms Decision Tree on ROC-AUC by {(roc_auc_lgb - roc_auc_dt):.3f} ({((roc_auc_lgb - roc_auc_dt) / roc_auc_dt * 100):.1f}% improvement)\")\n",
        "else:\n",
        "    print(f\"✗ Decision Tree outperforms LightGBM on ROC-AUC by {(roc_auc_dt - roc_auc_lgb):.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "save-header",
      "metadata": {},
      "source": [
        "### Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "save-model",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Best LightGBM model saved as 'best_lightgbm_model.pkl'\n",
            "✓ Model info saved as 'lightgbm_model_info.pkl'\n"
          ]
        }
      ],
      "source": [
        "# Save the best LightGBM model\n",
        "joblib.dump(best_lgb_model, './models/best_lightgbm_model.pkl')\n",
        "print(\"✓ Best LightGBM model saved as 'best_lightgbm_model.pkl'\")\n",
        "\n",
        "# Also save the model parameters for reference\n",
        "model_info = {\n",
        "    'best_params': lgb_grid_search.best_params_,\n",
        "    'best_cv_score': lgb_grid_search.best_score_,\n",
        "    'test_metrics': {\n",
        "        'accuracy': accuracy_lgb,\n",
        "        'precision': precision_lgb,\n",
        "        'recall': recall_lgb,\n",
        "        'f1_score': f1_lgb,\n",
        "        'roc_auc': roc_auc_lgb\n",
        "    },\n",
        "    'scale_pos_weight': scale_pos_weight_value\n",
        "}\n",
        "\n",
        "joblib.dump(model_info, './models/lightgbm_model_info.pkl')\n",
        "print(\"✓ Model info saved as 'lightgbm_model_info.pkl'\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
